============
INTRODUCTION
============

"Intelligence and artificial intelligence is data compression through laws that predict based on data correlations."

Neural networks are part of machine learning wich is part of AI which is part of computer science.

Neural networks become practical when data is not linearly separable and consists of lots of parameters, 
pratical for image recognition for example whereby at least each grayscaled pixel forms a parameter input. 
Can be used for both classification(choose a label) or regression(choose a quantity).

They consist of multiple node layers:
 - First layer takes parameter inputs and is also called input layer
 - Last layer making predictions also called output layer
 - Layers in between called deep layers and are optional. Once deep layers are introduced the data becomes non-linearly separable and the name "deep learning" can be used.

An AI will:
- Predict: Make a prediction based on input and given weights, in NN this process is called forward propagation
- Evaluate: Evaluate prediction compared to expected output, calculate total error and use cost function
- Adapt: Change weights of neural network to limit the total error, in NN this process is called backward propagation

===================
MATH IMPLEMENTATION
===================
Imagine a network with 4 layers, first is 0 and last 3


L layers
B bias, additional node in layer that is not connected to other nodes and always equals to one, 
because it always equals to one we will use it to indicate its weight value that determines its final value
I inputs == L0
W equals to weights layer
Z equals layer/node output before activation function
A equals layer/node output after activation function
g equals activation function, output layer can have own activation function different from main activation function
g' equals activation function derivative
Y equals to expected output
Yhat or predicted output
D or delta is a measure of error for each layer's final activation value used in backpropagation
d indicates partial derivative which is same as gradient
@ equals dot product
TE is total error of NN output
.T is the vector/matrix transposed to allow for dot product calculations
C or cost function is used to compute total error of each output node

Vectors representing complete layers can be used to make calculations more efficiently using numpy.

FORWARD PROPAGATION
-------------------

L0 = I

Z1 = (L0 @ W0) + B0
A1 = g(Z1)
L1 = A1

Z2 = (L1 @ W1) + B1
A2 = g(Z2)
L2 = A2

Z3 = (L2 @ W2) + B2
A3 = g(Z3)
L3 = A3

Yhat = L3

CALCULATE TOTAL ERROR
---------------------

TE = C(Y, Yhat)

BACK PROPAGATION
----------------

D3 = TE
d(W2) = A2.T @ D3
d(B2) = D3

D2 = W2.T @ D3 * g'(A2)
d(W1) = A1.T @ D2
d(B1) = D2

D1 = W1.T @ D2 * g'(A1)
d(W0) = A0.T @ D1
d(B0) = D1

W0 -= d(W0)
D0 -= d(D0)
W1 -= d(W1)
D1 -= d(D1)
W2 -= d(W2)
D2 -= d(D2)

=========================
NEURAL NETWORK PARAMETERS
=========================

LAYERS & NODES
--------------
Contains at least an input layer and ouptput layer. Deep layers sit in between. Each layer contains a certain amount of nodes.

If the data is linearly separable, you do not need any deep layers. Deep layers allow for non-linearity like polynomials would, when polynomials get too complicated neural networks come in. One layer is similar to linear/logistic regression without polynomials.

In general one hidden layer is sufficient for the majority of problems. The word deep learning a sub-field of machine learnign refers to this.

More deep layers increase the complexity of the neural net which increases computational cost and slows down convergence, but they can improve precision, sometimes too much whereby they create overfitting if data is scarce.

For the number of nodes per layer a pyramid structure is used, whereby the number of nodes is highest at input each following deep layer is lower than the prior one and lowest at ouptut.

LEARNING RATE
-------------
Test to find out what learning rate is best, default learning rate used is 0,01.
Learning rate is denoted as alpha.

When alpha is too small algorithm needs to perform more steps until convergence and become slower.
When alpha is too big potentially no convergence or less precision as it will hover over the minima.

GRADIENT DESCEND
----------------
Stochastic:
Faster convergence on small datasets but slower on big datasets due to constant weight update
Can avoid local minimas or premature convergence but has higher variance in results due to randomness

Batch:
Slow but more computanional efficient on big datasets
Stable convergence but risk of local minima or premature convergence

Mini-batch:
Mini-batch sits between stochastic and batch, trying to optimize benefits of both, and is the recommended variant of gradient descend. 
b variable in NN holds size of batch, often 32 is used as default, some sources recommend number between 2 and 32...

ACTVATION FUNCTION
------------------
Linear: output -inf,inf
ReLU: rectified linear unit, output 0,+inf, less sensitive to vanishing gradient and non-relevant nodes, less computational cost, most used
Tanh: hyperbolic tangent function, output -1,1, could converge faster on larger dataset than sigmoid
Sigmoid: ouput 0,1
Softmax: vector total output = 1

~ OUTPUT LAYER ~
Regression -> Linear or ReLU
Binary classification or multiple classes with potential multiple correct answers -> sigmoid
Single answer possible from multiple classes -> softmax

~ DEEP LAYER ~
ReLU, Tanh or sigmoid
Can all be tried in following order: ReLu, Tanh, sigmoid

COST FUNCTION
-------------
Is used to calculate total error, total error is used to indicate NN performance and in backpropagation to adjust the weights and bias accordingly.
Regression -> mean square error (MSE)
classification -> cross entropy

WEIGHT & BIAS INIT
------------------
Weights  intialization is based on deep layer activation function:
ReLU -> He init
Tanh -> Xavier init
sigmoid -> random init (default init) -> between -1,1

Init to zero is also possible if bias is not equal to zero, but is not optimal.

Optimizing init is practical to fasten convergence by avoiding vanishing gradient problem

Bias are usually init to 0, starting of neutral.

TERMS MORE EXPLAINED
--------------------
Vanishing gradient problem -> small values are slow to change/learn, leading to no/slow convergence, problem when weights are initialized to zero for example
non-relevant nodes -> Some nodes that are not relevant should be deactivated by the activation function setting its value to 0. ReLU does that best.

================
DATA PREPARATION
================

y or predicted values and x or features should be separated.
Data can be normalied meaning transforming all data into 0,1 range, faster convergence by reducing computational cost through samll numbers, also sets different features on same scale
Data can further be split into training data and test data (0.8 - 0.2 recommended ratio), to verify overfitting. Also possible training, test and validation set (0.6, 0.2, 0.2).
Features with textual data can be converted into numerical data, each label takes different number.
If your NN has multiple output nodes, the y or expected vaalues column, should be transformed not in single values but in vectors with same size as output nodes.

DESCRIBE
--------
Describe function goes over each feature in data and looks at different analytical parameters:
-See if data is correct in terms of numbers
-See if data needs to be normalized? Big values or already small? Different features same scale?
-Are there any missing datas?
-Skewness result far away from 0 means alot of skewness, not good data feature for AI
-Kurtosis result high number means the dataset has lots of outliers not good for AI (outliers can be removed)

PAIRPLOT
--------
Pairplots compares two features over the different classes, in a line (separate comparison) plot and scatterplot:
-Scatterplots are useful to find correlations and homogenousity between two features.
If one of two features are the same, one of them is not interesting for AI and can be eliminated.
-Line plots are useful to find correlations between classes in one feature
Features that are homogenous or have low variation over the classes are not interesting for AI neither as they have low predictive power

SOURCES
-------

Nerual networks, a visual introduction for beginners - Michael Taylor

https://www.coursera.org/learn/machine-learning/ - Andrew Ng
https://course.elementsofai.com/

https://towardsdatascience.com/
https://www.machinecurve.com/
https://www.deeplearning.ai/
https://machinelearningmastery.com/

